<!DOCTYPE html>
<html lang="en"><head>
    <link rel="apple-touch-icon" sizes="180x180" href="https://dbrg77.github.io/SUSTech-BIO210/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://dbrg77.github.io/SUSTech-BIO210/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://dbrg77.github.io/SUSTech-BIO210/favicon-16x16.png">
    <link rel="manifest" href="https://dbrg77.github.io/SUSTech-BIO210/site.webmanifest">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <style>
        :root {
            --accent-color: #FF4D4D;
        }
    </style>

    
    
    
    
    
    

    
    <title>Lecture 17 Maximum Likelihood Estimation (MLE)</title>
    <meta name="description" content="by School of Life Sciences at SUSTech">
    <meta name="keywords" content='blog, gokarna, hugo, data science, biology, genomics'>

    <meta property="og:url" content="https://dbrg77.github.io/SUSTech-BIO210/posts/lecture-17/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Lecture 17 Maximum Likelihood Estimation (MLE)">
    <meta property="og:description" content="by School of Life Sciences at SUSTech">
    <meta property="og:image" content="/images/avatar.jpg">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Lecture 17 Maximum Likelihood Estimation (MLE)">
    <meta name="twitter:description" content="by School of Life Sciences at SUSTech">
    <meta property="twitter:domain" content="https://dbrg77.github.io/SUSTech-BIO210/posts/lecture-17/">
    <meta property="twitter:url" content="https://dbrg77.github.io/SUSTech-BIO210/posts/lecture-17/">
    <meta name="twitter:image" content="/images/avatar.jpg">

    
    <link rel="canonical" href="https://dbrg77.github.io/SUSTech-BIO210/posts/lecture-17/" />

    <link rel="stylesheet" type="text/css" href="https://dbrg77.github.io/SUSTech-BIO210/css/normalize.min.css" media="print" onload="this.media='all'">
    <link rel="stylesheet" type="text/css" href="https://dbrg77.github.io/SUSTech-BIO210/css/main.css">
    <link disabled id="dark-theme" rel="stylesheet" href="https://dbrg77.github.io/SUSTech-BIO210/css/dark.css">

    <script src="https://dbrg77.github.io/SUSTech-BIO210/js/svg-injector.min.js"></script>
    <script src="https://dbrg77.github.io/SUSTech-BIO210/js/feather-icons.min.js"></script>
    <script src="https://dbrg77.github.io/SUSTech-BIO210/js/main.js"></script>

    
    
        <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.16/dist/katex.min.css" integrity="sha384-6LkG2wmY8FK9E0vU9OOr8UvLwsaqUg9SETfpq4uTCN1agNe8HRdE9ABlk+fVx6gZ" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.16/dist/katex.min.js" integrity="sha384-31El76TwmbHj4rF9DyLsygbq6xoIobG0W+jqXim+a3dU9W53tdH3A/ngRPxOzzaB" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.16/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>
  
    
</head>
<body>
        <script type="text/javascript">
            
            setThemeByUserPref();
        </script><header class="header">
    <nav class="header-nav">

        
        <div class="avatar">
            <a href="https://dbrg77.github.io/SUSTech-BIO210/">
                <img src="https://dbrg77.github.io/SUSTech-BIO210//images/avatar.jpg" alt="avatar" />
            </a>
        </div>
        

        <div class="nav-title">
            <a class="nav-brand" href="https://dbrg77.github.io/SUSTech-BIO210/">SUSTech BIO210 Biostatistics</a>
        </div>

        <div class="nav-links">
            
            <div class="nav-link">
                <a href="https://dbrg77.github.io/SUSTech-BIO210"><span data-feather='home'></span> Home </a>
            </div>
            
            <div class="nav-link">
                <a href="https://dbrg77.github.io/SUSTech-BIO210/posts/"><span data-feather='book-open'></span> Posts </a>
            </div>
            
            <div class="nav-link">
                <a href="https://dbrg77.github.io/SUSTech-BIO210/course/"><span data-feather='list'></span> Content Index </a>
            </div>
            
            <div class="nav-link">
                <a href="https://dbrg77.github.io/SUSTech-BIO210/about/"><span data-feather='file-text'></span> About </a>
            </div>
            
            <div class="nav-link">
                <a href="https://github.com/dbrg77/SUSTech-BIO210"><span data-feather='github'></span>  </a>
            </div>
            

            <span class="nav-icons-divider"></span>
            <div class="nav-link dark-theme-toggle">
                <span id="dark-theme-toggle-screen-reader-target" class="sr-only"></span>
                <a>
                    <span id="theme-toggle-icon" data-feather="moon"></span>
                </a>
            </div>

            <div class="nav-link" id="hamburger-menu-toggle">
                <span id="hamburger-menu-toggle-screen-reader-target" class="sr-only">menu</span>
                <a>
                    <span data-feather="menu"></span>
                </a>
            </div>

            
            <ul class="nav-hamburger-list visibility-hidden">
                
                <li class="nav-item">
                    <a href="https://dbrg77.github.io/SUSTech-BIO210"><span data-feather='home'></span> Home </a>
                </li>
                
                <li class="nav-item">
                    <a href="https://dbrg77.github.io/SUSTech-BIO210/posts/"><span data-feather='book-open'></span> Posts </a>
                </li>
                
                <li class="nav-item">
                    <a href="https://dbrg77.github.io/SUSTech-BIO210/course/"><span data-feather='list'></span> Content Index </a>
                </li>
                
                <li class="nav-item">
                    <a href="https://dbrg77.github.io/SUSTech-BIO210/about/"><span data-feather='file-text'></span> About </a>
                </li>
                
                <li class="nav-item">
                    <a href="https://github.com/dbrg77/SUSTech-BIO210"><span data-feather='github'></span>  </a>
                </li>
                
                <li class="nav-item dark-theme-toggle">
                    <span id="dark-theme-toggle-screen-reader-target" class="sr-only">theme</span>
                    <a>
                        <span id="theme-toggle-icon" data-feather="moon"></span>
                    </a>
                </li>
            </ul>

        </div>
    </nav>
</header>
<main id="content">
    <div class="post container">
    <div class="post-header-section">
        <h1>Lecture 17 Maximum Likelihood Estimation (MLE)</h1>
        <small role="doc-subtitle"></small>
        <p class="post-date">
            October 23, 2023
        </p>

        <ul class="post-tags">
        
        </ul>
    </div>

    <div class="post-content">
        <p>
            <p>Previously we all assumed that we somehow knew the population parameters, such as the mean ($\mu$) and the variance ($\sigma^2$). Starting this lecture, we have finally reached a stage where we do NOT know the population parameters. Instead, we have a representative sample. Using the information from the sample, we &ldquo;make a guess&rdquo;, or <strong>estimate</strong> the parameters of the population.</p>
<p>In the simplest case, if we are interested in the population mean $\mu$ or variance $\sigma^2$, we can just provide a number to represent our best &ldquo;guess&rdquo; or <strong>estimate</strong> to the mean or variance. This is called the <strong>point estimation</strong>. How the question becomes: how do we provide a reasonable estimate Intuitively, we probably will use the sample mean $\bar{x}$ as the estimate for the population mean $\mu$ and the sample variance $s^2$ the population variance $\sigma^2$. It is consistent with our common sense, but why does it work?</p>
<p>Let&rsquo;s set that question aside and introduce a systematic way of making an estimate.</p>
<h2 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h2>
<p>In this lecture we will introduce an intuitive way of making an estimate: the <strong>maximum likelihood estimation (MLE)</strong>. The basic idea of <strong>MLE</strong> is pretty straightforward to understand. Once again, let&rsquo;s stick to our basic principle: <strong>whenever we start to do something new, <em>always</em>, <em>always</em> start with something simple to get an intuition.</strong> We can start with a coin flipping example:</p>
<ul>
<li>We have a coin with an unknown $\mathbb{P}(H)$. The coin is then flipped 10 times, independently. The result turns out to be &ldquo;7 Hs and 3 Ts&rdquo;. Based on this result we want to infer $\mathbb{P}(H)$.</li>
</ul>
<p>We may never be sure about the true value of $\mathbb{P}(H)$. However, we have learnt quite a lot in the probability section. Using those knowledge, we can easily see that $0.5$ is more likely to be the true value of $\mathbb{P}(H)$ compared to, say, $0.00001$. I think you see the point here. Even though we may never be sure about the true value of $\mathbb{P}(H)$, what we can do is to figure out a value of $\mathbb{P}(H)$ between 0 and 1 such that it maximise our probability, or I should use the word <strong>likelihood</strong> (explained later), to observe the result &ldquo;7 Hs and 3 Ts&rdquo;. We use that value as our best guess, or the estimate, for $\mathbb{P}(H)$.</p>
<p>That&rsquo;s it. That is the basic idea of maximum likelihood estimation.</p>
<h2 id="the-likelihood-function">The Likelihood Function</h2>
<p>Now that we get an intuition of the basic idea of MLE, we need to formalise it and solve it in a quantitative and systematic way.</p>
<p>Suppose we have some observations $x_1,x_2,\cdots,x_{n-1},x_n$ from a random sample, that is, $n$ <strong>i.i.d.</strong> random variables $X_1,X_2,\cdots,X_{n-1},X_n$ drawn from a population with some unknown parameter(s) $\theta$. We define a <strong>likelihood function</strong> denoted by:</p>
<p>$$\mathcal{L}(\theta;x_1,x_2,\cdots,x_{n-1},x_n)$$</p>
<p>It means the likelihood of the parameter(s) takes the value(s) $\theta$, given that we have observed $x_1,x_2,\cdots,x_{n-1},x_n$. If we are dealing with a discrete case, the likelihood function is defined as:</p>
<p>$$\mathcal{L}(\theta;x_1,x_2,\cdots,x_{n-1},x_n) = \mathbb{P}(X_1=x_1,X_2=x_2,\cdots,X_{n-1}=x_{n-1},X_n=x_n)$$</p>
<p>Before we go further, let&rsquo;s have a look at the meaning of the likelihood function. In the discrete case, the likelihood is defined as the probability of observing $X_1$ takes the value $x_1$ AND $X_2$ takes the value $x_2$ &hellip; AND $X_n$ takes the value $x_n$. <strong>Therefore, our task becomes finding the value(s) of $\theta$ that maximise the likelihood function $\mathcal{L}$.</strong></p>
<p>Since $X_1,X_2,\cdots,X_{n-1},X_n$ are <strong>i.i.d.</strong>, the above equation becomes:</p>
<p>$$
\begin{aligned}
\mathcal{L}(\theta;x_1,x_2,\cdots,x_{n-1},x_n) &amp;= \mathbb{P}(X_1 = x_1) \cdot \mathbb{P}(X_2 = x_2) \cdot \cdots \cdot \mathbb{P}(X_n=x_n) \\
&amp;= \mathbb{P}(x_1;\theta) \cdot \mathbb{P}(x_2;\theta) \cdot \cdots \cdot \mathbb{P}(x_n;\theta)\\
&amp;= \prod_{i=1}^{n}\mathbb{P}(x_i;\theta)
\end{aligned}
$$</p>
<p>where $\mathbb{P}(x_i;\theta)$ is the PMF of each $X_i$. They are all the same as the population.</p>
<p>Similarly, when we are dealing with continuous cases, we replace the PMF with PDF. Therefore, the likelihood function is generally written as follows, regardless of the type of the random variable:</p>
<p>$$\mathcal{L}(\theta;x_1,x_2,\cdots,x_{n-1},x_n) = \prod_{i=1}^{n}f(x_i;\theta)$$</p>
<p>The multiplication in the formula is kind of annoying to compute. In practice, we often look at the <strong>log likelihood function</strong>, which is the logarithm of $\mathcal{L}$. The base is not important, but we often use the natural log:</p>
<p>$$\ell(\theta;x_1,x_2,\cdots,x_{n-1},x_n) = \ln\mathcal{L} = \ln\prod_{i=1}^{n}f(x_i;\theta) = \sum_{i=1}^{n}\ln f(x_i;\theta)$$</p>
<p>Since the <strong>monotonic increasing</strong> nature of the logarithm, when $\ell$ takes the maximum, $\mathcal{L}$ also takes the maximum. Therefore, <strong>in practice we often want to find the value(s) of $\theta$ that maximise the log likelihood function $\ell$.</strong> How do we do that then? We let the derivative:</p>
<p>$$\cfrac{\mathrm{d}\ell}{\mathrm{d}\theta} = 0$$</p>
<p>and we solve for $\theta$. We make sure the derivative at the left-hand side of the solution is positive AND the right-hand side negative. Then we know that $\ell$, and hence $\mathcal{L}$ takes the maximum at the solution. We use a few examples during the lectures to demonstrate the method and you will also be asked to practice in the homework. They are all relatively easy to to compute by hand. The purpose is to get you familiar with the idea, not the computation <em>per se</em>.</p>
<h2 id="notations--terminologies">Notations &amp; Terminologies</h2>
<h3 id="the-symbols--vs-">The symbols &ldquo;$;$&rdquo; vs &ldquo;$|$&rdquo;</h3>
<p>You sometimes will see the likelihood function written like this:</p>
<p>$$\mathcal{L}(\theta|x_1,x_2,\cdots,x_{n-1},x_n) = \prod_{i=1}^{n}f(x_i|\theta)$$</p>
<p>They have the same meaning as what we are introducing here. We choose to use the semicolon &ldquo;$;$&rdquo; instead of using the pipe &ldquo;$|$&rdquo;. The reason is that we already used the pipe symbol $|$ in conditional probability, such as $\mathbb{P}(A|B)$. You see, what follows a pipe symbol is a probabilistic event. Here, we are talking about parameters ($\theta$) of a distribution. The parameters are not really probabilistic events. Therefore, we stick to using the semicolon &ldquo;$;$&rdquo;.</p>
<h3 id="derivative-notations">Derivative Notations</h3>
<p>When we are dealing with derivatives, there are generally three different notations:</p>
<p><strong>1. Lagrange&rsquo;s Notation</strong></p>
<p>This is the one we first learnt in our hight school: $f^{\prime}$, read as &ldquo;f prime&rdquo;. The advantage of this notation is its simplicity. It is most commonly seen in single-variable functions. The second, third, &hellip; derivatives can be simply written as $f^{\prime\prime}$, $f^{\prime\prime\prime}$ <em>etc.</em></p>
<p><strong>2. Leibniz&rsquo;s Notation</strong></p>
<p>The derivative for $y=f(x)$ is expressed as: $\cfrac{\mathrm{d}y}{\mathrm{d}x}$, read as &ldquo;d y d x&rdquo;. We can also write it as: $\cfrac{\mathrm{d}f(x)}{\mathrm{d}x}$ or $\mathrm{d}\cfrac{f(x)}{\mathrm{d}x}$. The advantage of this notation is that it is very clear on what the variable is when we are taking the derivative. This is especially helpful when dealing with multi-variable functions, such as:</p>
<p>$$z = x^2 + y^2$$</p>
<p>If we do a partial derivative with respect to $x$, we write $\cfrac{\partial z}{\partial x}$; if we do a partial derivative with respect to $y$, we write $\cfrac{\partial z}{\partial y}$. The variable in each case is very clear to us. Note that the $\partial$ symbol has the same meaning with $\mathrm{d}$ in the single variable case. It is just that when we do a derivate on multi-variable functions, we use $\partial$.</p>
<p><strong>3. Newton&rsquo;s notation</strong></p>
<p>It is as $\dot y$, $\dot f$ <em>etc.</em> which seems to be used in certain areas of research.</p>
<h3 id="estimator--estimate">Estimator &amp; Estimate</h3>
<p>We put a hat symbol &quot; $\hat{}$ &quot; on top of a parameter to denote the <strong>estimator</strong> or <strong>estimate</strong> of the parameter. That is, $\hat{\theta}$ is an estimator or estimate of $\theta$. Use the population mean $\mu$ as an example, $\hat{\mu}$ is an estimator or estimate of $\mu$. During the lecture, we see that with maximum likelihood estimation, we have:</p>
<p>$$\boldsymbol{\hat{\mu}} = \cfrac{1}{n}\sum_{i=1}^{n}X_i \textmd { or } \hat{\mu} = \cfrac{1}{n}\sum_{i=1}^{n}x_i$$</p>
<p>On the left, it is an <strong>estimator</strong>, which contains instructions of making an estimation. Apparently, an estimator is a random variable. If an estimator is obtained using maximum likelihood estimation, then it is a <strong>maximum likelihood estimator</strong>. On the right, it is an <strong>estimate</strong>. That is, if you follow the instruction in the estimator by putting the values in, you will get a number out, which is the estimate. You can see that an estimate is just the number that the estimator takes.</p>
<p>When we have an estimator $\boldsymbol{\hat{\theta}}$, if $E[\boldsymbol{\hat{\theta}}]=\theta$, we say $\boldsymbol{\hat{\theta}}$ is an <strong>unbiased</strong> estimator for the parameter $\theta$.</p>
<h3 id="likelihood-vs-probability">Likelihood vs Probability</h3>
<p>Most of the time, those two terms are used interchangeably. However, statisticians like to make distinctions about them. We use <strong>probability</strong> to talk about probabilistic events. The word <strong>likelihood</strong> is often used to describe probability distributions and models, which very often involves in figuring out some parameters of the distribution. I also quote the definition from <a href="https://mathworld.wolfram.com/Likelihood.html">Wolfram</a>:</p>
<blockquote>
<p>Likelihood is the hypothetical probability that an event that has already occurred would yield a specific outcome. The concept differs from that of a probability in that a probability refers to the occurrence of future events, while a likelihood refers to past events with known outcomes.</p>
</blockquote>


<div class="admonition tip">
    <div class="title">References</div>
    <div class="content"><ul>
<li><a href="https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability">Probability vs. Likelihood</a></li>
<li><a href="https://brilliant.org/wiki/maximum-likelihood-estimation-mle">Maximum Likelihood Estimation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">Bias of an estimator</a></li>
<li><a href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-1/a/derivative-notation-review">Derivative notations</a></li>
</ul>
</div>
</div>

        </p>
    </div>

    <div class="prev-next">
        
    </div>
</div>

<aside class="post-toc">
    <nav id="toc">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</a></li>
    <li><a href="#the-likelihood-function">The Likelihood Function</a></li>
    <li><a href="#notations--terminologies">Notations &amp; Terminologies</a>
      <ul>
        <li><a href="#the-symbols--vs-">The symbols &ldquo;$;$&rdquo; vs &ldquo;$|$&rdquo;</a></li>
        <li><a href="#derivative-notations">Derivative Notations</a></li>
        <li><a href="#estimator--estimate">Estimator &amp; Estimate</a></li>
        <li><a href="#likelihood-vs-probability">Likelihood vs Probability</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </nav>
</aside>



    

        </main><footer class="footer">
    <span>&copy; 2023 SUSTech BIO210 Biostatistics</span>
    <span>
        Made with &#10084;&#65039; using <a target="_blank" href="https://github.com/526avijitgupta/gokarna">Gokarna</a>
    </span>
</footer>
</body>
</html>
