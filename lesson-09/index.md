# Week 6, Friday


## Friday, 2022-Mar-25
Although we used the __sample mean ($\bar{x}$)__ and the __sample variance ($s^2$)__ as the estimates for the __population mean ($\mu$)__ and the __population variance ($\sigma^2$)__, they are based on our intuition. We have never really talked about how to construct an __estimator__ to help us make the estiamtion. On Friday, we will introduce probably the most intuitive way of constructing an estimator: the __maximum likelihood estimator__. We will talk about what __maximum likelihood etimation (MLE)__ is, the intuition about MLE and the advantages/limitations of MLE. Then, we will discuss how to construct maximum likelihood estimators for some common PMFs and PDFs. In addition, we will have a look at a thing that I promised during the desciriptive statistics section. That is, why do we use the following way of calculating the __sample variance ($s^2$)__:

$$s^2=\cfrac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2$$

We will explain why using $n-1$ here, and briefly mention __degree of freedom__. The concept of __degree of freedom__ is complicated even experts have trouble explaining it. Luckily, it is relatively easy in the context of sample variance calculation. We will talk a bit more about this later on.

#### Lecture slides
- [Lecture 17 Maximum Likelihood Estimation (MLE)](/lecture_slides/Lecture_17_Maximum_Likelihood_Estimation_handout.pdf)
- [Lecture 18 More On Maximum Likelihood Estimation](/lecture_slides/Lecture_18_More_On_Maximum_Likelihood_Estimation_handout.pdf)

#### Extra reading material
- [MLE for Variance](/lecture_slides/Lecture_18_MLE_For_Variance.pdf)

#### Homework assignment
- [Assignment 5](/assignments/Assignment_5.pdf)

#### Interesting links
- [Degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics))
- [Maximum Likelihood Estimation](https://brilliant.org/wiki/maximum-likelihood-estimation-mle)
- [Bias of an estimator](https://en.wikipedia.org/wiki/Bias_of_an_estimator)

